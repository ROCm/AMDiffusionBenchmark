{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from src.constants import METADATA_COLUMNS, METRIC_COLUMNS\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-dark\")\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \".7\", \"grid.linestyle\": \":\"})\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "The output displays the DataFrame `df`, which contains all **successful runs** from the dataset. This DataFrame includes:\n",
    "\n",
    "- **run_name**: A unique identifier for each run, generated based on the combination of feature values.\n",
    "- **Features**: A list of features with more than one unique value, used for analysis.\n",
    "- **Performance Metrics**:\n",
    "  - `avg_fps`: Average frames per second achieved during the run.\n",
    "  - `std_fps`: Standard deviation of the frames per second, indicating performance consistency.\n",
    "  - `avg_loss`: Average loss achieved during the run.\n",
    "  - `std_loss`: Standard deviation of the loss, indicating performance consistency.\n",
    "  - `avg_loss_tail`: Average loss achieved over the last iterations of the run the run.\n",
    "  - `std_loss_tail`: Standard deviation of the `avg_loss`, indicating performance consistency.\n",
    "\n",
    "Additionally, the following information is printed:\n",
    "\n",
    "- **Features**: A list of all features considered for analysis.\n",
    "- **Binary Features**: A subset of features that have exactly two unique values.\n",
    "- **Run Counts**:\n",
    "  - **All runs**: Total number of runs in the dataset.\n",
    "  - **Successful runs**: Number of runs where `avg_fps` is not zero.\n",
    "  - **Failed runs**: Number of runs where `avg_fps` is zero (due to `error`, `oom`, `oom-skipped`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrames\n",
    "df_path = \"outputs/runs/sweep_000/runs_summary.csv\"\n",
    "df_raw = pd.read_csv(df_path)\n",
    "\n",
    "# Split df in successful and failed runs\n",
    "df_failed = df_raw[df_raw[\"status\"].isin([\"error\", \"oom\", \"oom-skipped\"])].copy()\n",
    "df = df_raw[df_raw[\"status\"].isin([\"success\"])].copy()\n",
    "\n",
    "# Convert sets to lists and concatenate\n",
    "excluded_columns = list(METRIC_COLUMNS) + list(METADATA_COLUMNS)\n",
    "\n",
    "# Automatically extract features from DataFrame columns\n",
    "all_columns = df.columns.tolist()\n",
    "features = [col for col in all_columns if col not in excluded_columns]\n",
    "\n",
    "# Update features to include only those with more than one unique value\n",
    "features = [feature for feature in features if df[feature].nunique() > 1]\n",
    "\n",
    "\n",
    "# Create df IDs\n",
    "def create_run_id(row):\n",
    "    return \"_\".join(\n",
    "        (\n",
    "            f\"{''.join(col.replace('.', '_').split('_')[i][0].lower() for i in range(len(col.replace('.', '_').split('_'))))}{val}\"\n",
    "        )\n",
    "        for col, val in row.items()\n",
    "    )\n",
    "\n",
    "\n",
    "df.loc[:, \"run_name\"] = df[features].apply(create_run_id, axis=1)\n",
    "\n",
    "# Define no-batch-size features\n",
    "no_bs_features = [f for f in features if f != \"train_batch_size\"]\n",
    "\n",
    "# Define binary_features\n",
    "binary_features = [feature for feature in features if df[feature].nunique() == 2]\n",
    "\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Binary features: {binary_features}\")\n",
    "\n",
    "# Sort the DataFrame by avg_fps in descending order\n",
    "df_sorted = df.sort_values(by=\"avg_fps\", ascending=False)\n",
    "\n",
    "# Display the DataFrame with run_name, features, avg_fps, and std_fps\n",
    "display_columns = [\"run_name\"] + features + list(METRIC_COLUMNS)\n",
    "\n",
    "print(f\"All runs: {len(df_raw)}\")\n",
    "print(f\"Successful runs: {len(df)}\")\n",
    "print(f\"Failed runs: {len(df_failed)}\")\n",
    "\n",
    "df_sorted[display_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique values from DataFrame for consistent plotting\n",
    "backend_order = sorted(df[\"dynamo_config.dynamo_backend\"].unique())\n",
    "precision_order = sorted(df[\"mixed_precision\"].unique())\n",
    "\n",
    "# Define colors using a colormap based on number of precision types\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(precision_order)))\n",
    "color_dict_precision = dict(zip(precision_order, colors))\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Group by backend first\n",
    "x_positions = []\n",
    "x_labels = []\n",
    "current_x = 0\n",
    "\n",
    "for backend in backend_order:\n",
    "    backend_data = df[df[\"dynamo_config.dynamo_backend\"] == backend]\n",
    "\n",
    "    # For each precision in this backend\n",
    "    for precision in precision_order:\n",
    "        precision_data = backend_data[backend_data[\"mixed_precision\"] == precision]\n",
    "\n",
    "        if not precision_data.empty:\n",
    "            # Get top performing run\n",
    "            top_run = precision_data.nlargest(1, \"avg_fps\").iloc[0]\n",
    "\n",
    "            # Plot bar\n",
    "            plt.bar(\n",
    "                current_x,\n",
    "                top_run[\"avg_fps\"],\n",
    "                color=color_dict_precision[precision],\n",
    "                yerr=top_run[\"std_fps\"],\n",
    "                capsize=5,\n",
    "            )\n",
    "\n",
    "            # Add value label on top of bar\n",
    "            plt.text(\n",
    "                current_x,\n",
    "                top_run[\"avg_fps\"],\n",
    "                f'{top_run[\"avg_fps\"]:.2f}\\n(bs={top_run[\"train_batch_size\"]})',\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "            x_positions.append(current_x)\n",
    "            x_labels.append(f\"{backend}\\n{precision}\")\n",
    "            current_x += 1\n",
    "\n",
    "    # Add space between backend groups\n",
    "    current_x += 0.5\n",
    "\n",
    "# Customize plot\n",
    "plt.xticks(x_positions, x_labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"FPS\")\n",
    "plt.title(\"Top Performing Configurations by Backend and Precision\")\n",
    "\n",
    "# Add legend for precision types\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor=color_dict_precision[prec], label=prec)\n",
    "    for prec in precision_order\n",
    "]\n",
    "plt.legend(handles=legend_elements, title=\"Precision\")\n",
    "\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed information about top configurations\n",
    "print(\"\\nTop configurations:\")\n",
    "for backend in backend_order:\n",
    "    print(f\"\\nBackend: {backend}\")\n",
    "    backend_data = df[df[\"dynamo_config.dynamo_backend\"] == backend]\n",
    "\n",
    "    for precision in precision_order:\n",
    "        precision_data = backend_data[backend_data[\"mixed_precision\"] == precision]\n",
    "        if not precision_data.empty:\n",
    "            top_run = precision_data.nlargest(1, \"avg_fps\").iloc[0]\n",
    "            print(f\"\\n{precision}:\")\n",
    "            print(f\"  FPS: {top_run['avg_fps']:.2f} Â± {top_run['std_fps']:.2f}\")\n",
    "            print(f\"  Batch size: {top_run['train_batch_size']}\")\n",
    "            print(f\"  Run name: {top_run['run_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS vs train_batch_size: Overview\n",
    "\n",
    "In this section, we'll examine the relationship between FPS and train_batch_size, while also considering the influence of data type (dtype) and Model Size. This visualization aims to provide insights into how these factors interact and affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "scatter = sns.scatterplot(\n",
    "    x=\"train_batch_size\",\n",
    "    y=\"avg_fps\",\n",
    "    hue=\"dynamo_config.dynamo_backend\",\n",
    "    style=\"mixed_precision\",\n",
    "    data=df,\n",
    "    palette=\"Set2\",\n",
    "    s=100,\n",
    ")\n",
    "\n",
    "scatter.set_title(\n",
    "    \"FPS vs train_batch_size, Colored by mixed_precision and Styled by use_cache\"\n",
    ")\n",
    "scatter.set_xlabel(\"train_batch_size\")\n",
    "scatter.set_ylabel(\"FPS\")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"mixed_precision and use_cache\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS vs train_batch_size: Scatter feature-wise Analysis\n",
    "\n",
    "This section presents an examination of how different features affect the relationship between FPS and train_batch_size. We've created a grid of scatter plots, each focusing on a specific feature's impact on performance.\n",
    "\n",
    "Key aspects of this visualization:\n",
    "- Grid Layout: The plots are arranged in a grid, with each subplot dedicated to a different feature.\n",
    "- Error Bars: Each data point includes error bars representing the standard deviation of FPS, giving insight into the variability of performance **within the same run**.\n",
    "- Color Coding: Different values of each feature are represented by distinct colors, allowing for easy comparison within each plot.\n",
    "\n",
    "Axes:\n",
    "- X-axis: train_batch_size\n",
    "- Y-axis: Average FPS\n",
    "\n",
    "This view allows us to:\n",
    "- Identify how each feature independently affects the FPS-train_batch_size relationship\n",
    "- Spot any features that have a more pronounced impact on performance\n",
    "- Detect potential interactions between features and train_batch_size\n",
    "- Observe the variability in performance for different feature values and train_batch_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows needed for the grid\n",
    "n_features = len(no_bs_features)\n",
    "n_rows = (n_features + 1) // 2  # Round up to the nearest integer\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(20, 8 * n_rows), constrained_layout=True)\n",
    "fig.suptitle(\"FPS vs train_batch_size, Faceted by Features\", fontsize=16, y=1.02)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "# Custom function to plot scatter points with error bars\n",
    "def scatter_with_errorbars(data, x, y, yerr, ax, hue, **kwargs):\n",
    "    for hue_val in data[hue].unique():\n",
    "        hue_data = data[data[hue] == hue_val]\n",
    "        ax.errorbar(\n",
    "            hue_data[x],\n",
    "            hue_data[y],\n",
    "            yerr=hue_data[yerr],\n",
    "            fmt=\"o\",\n",
    "            capsize=3,\n",
    "            label=hue_val,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "# Iterate through features and create a plot for each\n",
    "for i, feature in enumerate(no_bs_features):\n",
    "    if i < len(axes):\n",
    "        scatter_with_errorbars(\n",
    "            df,\n",
    "            x=\"train_batch_size\",\n",
    "            y=\"avg_fps\",\n",
    "            yerr=\"std_fps\",\n",
    "            ax=axes[i],\n",
    "            hue=feature,\n",
    "        )\n",
    "        axes[i].set_title(f\"FPS vs train_batch_size, by {feature}\")\n",
    "        axes[i].set_xlabel(\"train_batch_size\")\n",
    "        axes[i].set_ylabel(\"avg_fps\")\n",
    "        axes[i].legend(title=feature)\n",
    "        axes[i].grid(True, linestyle=\":\", alpha=0.7)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS vs train_batch_size: Averaged feature-wise Analysis\n",
    "\n",
    "This section presents an examination of how different features affect the relationship between FPS and train_batch_size. We've created a grid of point plots, each focusing on a specific feature's impact on performance.\n",
    "\n",
    "Key aspects of this visualization:\n",
    "- Grid Layout: The plots are arranged in a grid, with each subplot dedicated to a different feature.\n",
    "- Error Bars: Each data point includes error bars representing the standard deviation of FPS, giving insight into the variability of performance **between multiple runs**.\n",
    "- Color Coding: Different values of each feature are represented by distinct colors, allowing for easy comparison within each plot.\n",
    "\n",
    "Axes:\n",
    "- X-axis: train_batch_size\n",
    "- Y-axis: Average FPS\n",
    "\n",
    "This view allows us to:\n",
    "- Identify how each feature independently affects the FPS-train_batch_size relationship\n",
    "- Spot any features that have a more pronounced impact on performance\n",
    "- Detect potential interactions between features and train_batch_size\n",
    "- Observe the variability in performance for different feature values and train_batch_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(\n",
    "    2,\n",
    "    (len(no_bs_features) // 2 + len(no_bs_features) % 2),\n",
    "    figsize=(30, 10 * (len(no_bs_features) // 2 + len(no_bs_features) % 2)),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "fig.suptitle(\n",
    "    \"FPS over train_batch_size\",\n",
    "    fontsize=20,\n",
    "    y=1.02,\n",
    ")\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through no_bs_features and create a plot for each\n",
    "for i, feature in enumerate(no_bs_features):\n",
    "    sns.pointplot(\n",
    "        x=\"train_batch_size\",\n",
    "        y=\"avg_fps\",\n",
    "        hue=feature,\n",
    "        data=df,\n",
    "        errorbar=\"sd\",\n",
    "        dodge=True,\n",
    "        capsize=0.1,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "\n",
    "    axes[i].set_title(f\"Effect of {feature} on FPS\", fontsize=16)\n",
    "    axes[i].set_xlabel(\"train_batch_size\", fontsize=14)\n",
    "    axes[i].set_ylabel(\"avg_fps\", fontsize=14)\n",
    "    axes[i].legend(title=feature, loc=\"lower right\", fontsize=12, title_fontsize=14)\n",
    "    axes[i].grid(True, linestyle=\":\", alpha=0.7)\n",
    "\n",
    "    # Get current tick positions and labels\n",
    "    ticks = axes[i].get_xticks()\n",
    "    labels = axes[i].get_xticklabels()\n",
    "\n",
    "    # Set ticks first, then labels with rotation\n",
    "    axes[i].set_xticks(ticks)\n",
    "    axes[i].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS distribution by feature: violin plots\n",
    "\n",
    "This visualization presents an examination of how different features affect both the average and standard deviation of FPS. The plot is organized as a grid of violin plots and stripplots, providing a multi-faceted view of the data.\n",
    "The plots are arranged in a 2 x N grid, where N is the number of features. The top row shows the effect on average FPS, while the bottom row shows the effect on standard deviation of FPS.\n",
    "\n",
    "Dual Plot Type: Each cell in the grid contains two plot types overlaid:\n",
    "- A violin showing the distribution of FPS values\n",
    "- A stripplot showing individual data points\n",
    "\n",
    "Axes:\n",
    "- X-axis: Different categories of each feature\n",
    "- Y-axis: FPS values (Average or Standard Deviation)\n",
    "\n",
    "Violin Plot Details:\n",
    "- The violin plots are cut at the extremes of the data\n",
    "- Inner quartiles are displayed within the violin plots\n",
    "\n",
    "This visualization allows us to:\n",
    "- Compare the distribution of FPS across different categories within each feature\n",
    "- Identify features that have a significant impact on both average FPS and its variability\n",
    "- Spot outliers and understand the spread of data points within each category\n",
    "- Observe potential patterns or trends across features\n",
    "\n",
    "By presenting both the average and standard deviation of FPS, this plot provides insights into the performance distribution across different feature categories. This can be interesting for identifying configurations that offer both high and stable FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, len(no_bs_features), figsize=(10 * len(no_bs_features), 10), sharex=\"col\"\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(no_bs_features):\n",
    "    # Plot for avg_fps\n",
    "    sns.violinplot(\n",
    "        x=feature,\n",
    "        y=\"avg_fps\",\n",
    "        data=df,\n",
    "        ax=axes[0, i],\n",
    "        cut=0,\n",
    "        inner=\"quartile\",\n",
    "        hue=feature,\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        x=feature,\n",
    "        y=\"avg_fps\",\n",
    "        data=df,\n",
    "        color=\"black\",\n",
    "        alpha=0.7,\n",
    "        ax=axes[0, i],\n",
    "        jitter=True,\n",
    "    )\n",
    "    axes[0, i].set_title(f\"Effect of {feature} on avg_fps\")\n",
    "    axes[0, i].set_ylabel(\"avg_fps\")\n",
    "    axes[0, i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Plot for std_fps\n",
    "    sns.violinplot(\n",
    "        x=feature,\n",
    "        y=\"std_fps\",\n",
    "        data=df,\n",
    "        ax=axes[1, i],\n",
    "        cut=0,\n",
    "        inner=\"quartile\",\n",
    "        hue=feature,\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        x=feature,\n",
    "        y=\"std_fps\",\n",
    "        data=df,\n",
    "        color=\"black\",\n",
    "        alpha=0.7,\n",
    "        ax=axes[1, i],\n",
    "        jitter=True,\n",
    "    )\n",
    "    axes[1, i].set_title(f\"Effect of {feature} on std_fps\")\n",
    "    axes[1, i].set_xlabel(feature)\n",
    "    axes[1, i].set_ylabel(\"std_fps\")\n",
    "    axes[1, i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap: Average FPS Across Features vs train_batch_size\n",
    "\n",
    "This visualization presents a series of heatmaps examining the relationship between various features, train_batch_size, and average frames per second (FPS). The plot is organized as a grid of individual heatmaps, providing a comprehensive view of how different factors interact to affect performance.\n",
    "Key aspects of this visualization:\n",
    "The heatmaps are arranged in a grid, with each cell representing a different feature's interaction with train_batch_size.\n",
    "\n",
    "Heatmap Structure:\n",
    "- X-axis: Different categories or values of each feature\n",
    "- Y-axis: train_batch_size\n",
    "- Color scale: Represents the average FPS, with a \"coolwarm\" color scheme\n",
    "\n",
    "Data Representation:\n",
    "- Each cell in the heatmap shows the average FPS for a specific combination of feature value and train_batch_size\n",
    "- Actual FPS values are annotated within each cell for precise reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    len(no_bs_features) // 2 + len(no_bs_features) % 2,\n",
    "    2,\n",
    "    figsize=(20, 8 * (len(no_bs_features) // 2 + len(no_bs_features) % 2)),\n",
    ")\n",
    "fig.suptitle(\"Heatmaps of avg_fps Across no_bs_features\", fontsize=16, y=1.02)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through no_bs_features and create a heatmap for each\n",
    "for i, feature in enumerate(no_bs_features):\n",
    "    heatmap_data = df.pivot_table(\n",
    "        index=\"train_batch_size\", columns=feature, values=\"avg_fps\"\n",
    "    )\n",
    "\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=axes[i])\n",
    "\n",
    "    axes[i].set_title(f\"avg_fps: {feature} vs train_batch_size\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"train_batch_size\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Binary Features on Performance Metrics\n",
    "\n",
    "The plots illustrate how toggling each binary feature affects system performance, specifically focusing on the average frames per second (avg_fps) and the standard deviation of fps (std_fps).\n",
    "Here's what's being visualized:\n",
    "\n",
    "- **Comparison Between Feature States**: For each binary feature, the plots compare the two possible states (e.g., 0 and 1) while keeping all other feature settings constant.\n",
    "\n",
    "- **Top Plot (avg_fps Difference)**:\n",
    "  - **Y-Axis**: Displays the difference in avg_fps between the two states of the binary feature for each configuration.\n",
    "  - **X-Axis**: Each bar represents a unique configuration of the other features (denoted by `custom_id`).\n",
    "  - **Interpretation**: Positive bars indicate an increase in Avg fps when the feature is toggled, while negative bars indicate a decrease.\n",
    "\n",
    "- **Bottom Plot (std_fps Difference)**:\n",
    "  - **Y-Axis**: Shows the difference in std_fps between the two feature states for each configuration.\n",
    "  - **X-Axis**: Mirrors the top plot for direct comparison.\n",
    "  - **Interpretation**: Highlights how the feature toggle affects the consistency of performance.\n",
    "\n",
    "These plots help visualize how each binary feature influences performance metrics across different configurations, making it easier to identify features that have significant positive or negative effects on system performance.\n",
    "\n",
    "TODO: run comparison on execution time (1/FPS) rather than FPS more coherent reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_binary_feature(df, feature, features):\n",
    "    # Print DataFrame info for debugging\n",
    "    print(f\"Analyzing feature: {feature}\")\n",
    "    print(f\"Feature unique values: {df[feature].unique()}\")\n",
    "\n",
    "    # Create a custom ID excluding the current feature\n",
    "    other_features = [f for f in features if f != feature]\n",
    "    df[\"custom_id\"] = df[other_features].apply(create_run_id, axis=1)\n",
    "\n",
    "    # Find runs with both states for the current feature\n",
    "    feature_counts = df.groupby(\"custom_id\")[feature].nunique().reset_index()\n",
    "    matching_ids = feature_counts[feature_counts[feature] == 2][\"custom_id\"]\n",
    "\n",
    "    # Filter the dataframe for matching runs\n",
    "    df_matching = df[df[\"custom_id\"].isin(matching_ids)].copy()\n",
    "\n",
    "    if df_matching.empty:\n",
    "        print(f\"No matching data for feature: {feature}\")\n",
    "        return None\n",
    "\n",
    "    # Pivot the data for comparison\n",
    "    df_pivot = df_matching.pivot(\n",
    "        index=\"custom_id\", columns=feature, values=[\"avg_fps\", \"std_fps\"]\n",
    "    )\n",
    "    df_pivot.columns = [f\"{metric}_{value}\" for metric, value in df_pivot.columns]\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "\n",
    "    # Calculate differences\n",
    "    unique_values = sorted(df_matching[feature].unique())\n",
    "    if len(unique_values) < 2:\n",
    "        print(f\"Not enough unique values for feature: {feature}\")\n",
    "        return None\n",
    "\n",
    "    df_pivot[f\"avg_fps_diff\"] = (\n",
    "        df_pivot[f\"avg_fps_{unique_values[1]}\"]\n",
    "        - df_pivot[f\"avg_fps_{unique_values[0]}\"]\n",
    "    )\n",
    "    df_pivot[f\"std_fps_diff\"] = (\n",
    "        df_pivot[f\"std_fps_{unique_values[1]}\"]\n",
    "        - df_pivot[f\"std_fps_{unique_values[0]}\"]\n",
    "    )\n",
    "\n",
    "    return df_pivot, unique_values\n",
    "\n",
    "\n",
    "# Analyze each binary feature\n",
    "for feature in binary_features:\n",
    "    result = analyze_binary_feature(df, feature, features)\n",
    "\n",
    "    if result is not None:\n",
    "        df_pivot, unique_values = result\n",
    "\n",
    "        # Create plots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(25, 12))\n",
    "\n",
    "        # Plot avg_fps difference\n",
    "        sns.barplot(\n",
    "            x=\"index\",\n",
    "            y=\"avg_fps_diff\",\n",
    "            data=df_pivot.reset_index(),\n",
    "            hue=\"index\",\n",
    "            palette=\"Blues_d\",\n",
    "            legend=False,\n",
    "            ax=ax1,\n",
    "        )\n",
    "        ax1.set_title(\n",
    "            f\"Difference in avg_fps\\n{feature}: {unique_values[1]} vs {unique_values[0]}\"\n",
    "        )\n",
    "        ax1.set_ylabel(\"avg_fps Difference\")\n",
    "        ax1.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "        ax1.tick_params(axis=\"x\", which=\"both\", bottom=False, labelbottom=False)\n",
    "\n",
    "        # Plot std_fps difference\n",
    "        sns.barplot(\n",
    "            x=\"index\",\n",
    "            y=\"std_fps_diff\",\n",
    "            data=df_pivot.reset_index(),\n",
    "            hue=\"index\",\n",
    "            palette=\"Greens_d\",\n",
    "            legend=False,\n",
    "            ax=ax2,\n",
    "        )\n",
    "        ax2.set_title(\n",
    "            f\"Difference in std_fps\\n{feature}: {unique_values[1]} vs {unique_values[0]}\"\n",
    "        )\n",
    "        ax2.set_ylabel(\"std_fps Difference\")\n",
    "        ax2.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        # Set x-axis labels\n",
    "        x_ticks = np.arange(len(df_pivot))\n",
    "        ax2.set_xticks(x_ticks)\n",
    "        ax2.set_xticklabels(df_pivot[\"custom_id\"], rotation=90, ha=\"right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nAnalyzing feature: {feature}\")\n",
    "        print(f\"Unique values: {df[feature].unique()}\")\n",
    "        print(f\"Value counts:\\n{df[feature].value_counts()}\")\n",
    "        print(f\"Number of matching run_names: {len(df_pivot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps of Failed Runs Across Features\n",
    "\n",
    "The plots display heatmaps illustrating the number of failed runs across different configurations. Each heatmap corresponds to a specific feature (excluding \"train_batch_size\"), with:\n",
    "\n",
    "- **X-Axis**: Different values of the feature under consideration.\n",
    "- **Y-Axis**: Various \"train_batch_size\" settings.\n",
    "- **Color Intensity**: Represents the count of failed runs for each combination of \"train_batch_size\" and the feature value.\n",
    "\n",
    "These visualizations help identify patterns and correlations between feature settings, train_batch_sizes, and the frequency of failed runs, highlighting configurations that may lead to higher failure rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots (2 columns)\n",
    "fig, axes = plt.subplots(\n",
    "    len(no_bs_features) // 2 + len(no_bs_features) % 2,\n",
    "    2,\n",
    "    figsize=(20, 8 * (len(no_bs_features) // 2 + len(no_bs_features) % 2)),\n",
    ")\n",
    "fig.suptitle(\"Heatmaps of Failed Runs Across Features\", fontsize=16, y=1.02)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through features and create a heatmap for each\n",
    "for i, feature in enumerate(no_bs_features):\n",
    "    heatmap_data = df_failed.pivot_table(\n",
    "        index=\"train_batch_size\", columns=feature, aggfunc=\"size\", fill_value=0\n",
    "    )\n",
    "\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"YlOrRd\", fmt=\"d\", ax=axes[i])\n",
    "\n",
    "    axes[i].set_title(f\"Failed Runs: {feature} vs train_batch_size\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"train_batch_size\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Failed Runs Across Features\n",
    "\n",
    "The plots display histograms illustrating how failed runs are distributed across different feature values. Each subplot corresponds to a specific feature, showing the count of failed runs associated with each value or range of that feature.\n",
    "\n",
    "- **X-Axis**: Represents the values or categories of the feature being analyzed.\n",
    "- **Y-Axis**: Indicates the number of failed runs for each feature value.\n",
    "\n",
    "These visualizations help identify patterns and trends in the failure rates related to specific feature settings, making it easier to pinpoint features that may contribute to higher failure occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots (2 columns)\n",
    "fig, axes = plt.subplots(\n",
    "    len(features) // 2 + len(features) % 2,\n",
    "    2,\n",
    "    figsize=(20, 6 * (len(features) // 2 + len(features) % 2)),\n",
    ")\n",
    "fig.suptitle(\"Distribution of Failed Runs Across Features\", fontsize=16, y=1.02)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through features and create a histogram for each\n",
    "for i, feature in enumerate(features):\n",
    "    sns.histplot(df_failed[feature], bins=10, kde=False, ax=axes[i])\n",
    "\n",
    "    axes[i].set_title(f\"Failed Runs: {feature}\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Count of Failed Runs\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.dirname(df_path)\n",
    "notebook_name = 'analysis_notebook.ipynb'\n",
    "output_format = 'html'\n",
    "\n",
    "# If you want to set a custom output filename, e.g. analysis_notebook.html:\n",
    "output_file = os.path.join(output_dir, \"analysis_notebook.html\")\n",
    "\n",
    "# Export the notebook to HTML\n",
    "!jupyter nbconvert --to {output_format} --no-input --output {output_file} {notebook_name}\n",
    "\n",
    "print(f\"Successfully exported notebook to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
